# Логистическая регрессия

Что же, поигрались с пандами и нумпами и теперь приступаем к машинам. Я хочу начать с более сложной темы в виде логистической регрессии, НО! По опыту именно она после полного разбора даст вообще всю необходимую базу для машинки: что такое модель, что такое обучение, что значит с учителем и без, что за правдоподобие, что за гиперплоскость, что за линейные комбинации, что за байесы и дисперсии, короче вопросов много. Поэтому завариваем чай и приступаем к увлекательному путешествию в мир машин.



Начнем с вводных. Что такое линейная комбинация признаков? Это чудо очень пересекается с одним заданием в домашке по нумпаю и еще с тем, что мы разбирали в первый наш заход по многомерке.

&#x20;Сейчас будет мозгодробительная хератень, поэтому внимательно.

Начинаем с такой ситуации: у нас есть **ОДИН** объект (экземпляр, sample). У которого есть j (любое число, пусть 5 или 7) признаков. В виде таблицы это будет так:

| Экземпляры | Признак 1  | Признак 2  | ... | Признак j  |
| ---------- | ---------- | ---------- | --- | ---------- |
| Объект 1   | Значение 1 | Значение 2 | ... | Значение j |

Но такая запись никогда не используется практически, обычно все изображают в виде матрицы:

$$
\begin{pmatrix}
Значение ~1 & Значение ~2 & ... & Значение ~j \\
\end{pmatrix}
$$

Та же самая таблица (... обозначает, что опущено сколько-то столбцов), НО мы во первых не пишем объект 1 или объект 2, то есть строки ПОДРАЗУМЕВАЕТСЯ что обозначают объекты. Одна строка - один объект. То же самое и со столбцами. Мы не пишем признак 1, признак 2. ПОДРАЗУМЕВАЕТСЯ, что это признаки.

Теперь следующий уровень. В машинке и линале принято каждую строку (т.е. каждый экземпляр) обозначать как _**i**_, и говорят ИТЫЙ элемент/экземпляр. При этом МАКСИМАЛЬНОЕ количество экземпляров (строк) обозначается как _**m**_. Поэтому записывают _**i = 1, 2 , ... , m**_. То есть "у нас есть первый, второй... эм (m) экземпляров/строк".&#x20;

Такая же шняга с ПРИЗНАКАМИ (столбцами). Но там каждая ДЖИТЫЙ (_**j**_) элемент. Записывают _**j = 1, 2 , ... , n**_. То есть n это максимальное количество столбцов (признаков). Поэтому когда говорят, что у нас матрица m x n равная 3 x 2, то мы сразу понимаем, m = 3, n = 2. Значит у нас 3 экземпляра (строки) и 2 признака у КАЖДОГО экземпляра:

$$
\begin{pmatrix}
3 & 1  \\
5 & 2 \\
8 & 6 \\
\end{pmatrix}
$$

Но обычно в машинке для обозначения конкретного ЭЛЕМЕНТА в матрице используется буковка **x**. И при этом у нее записывается ее индекс (верхний и нижний):&#x20;

$$
x_j^{(i)}
$$

Что буквально обозначает: элемент икс в ИТОЙ строке и в ДЖИТОМ столбце. Вместо i и j подставляются определенные значения и таким образом мы получаем позицию элемента. Теперь переходим к вектору-СТРОКЕ (то есть мы взяли матрицу с названием X (да, матрицы обозначаются большими буковками) и вырезали у нее ИТУЮ строчку):&#x20;

$$
X^{(i)} = \begin{pmatrix}
x_1^{(i)} & x_2^{(i)} & x_3^{(i)} & x_4^{(i)}  \\
\end{pmatrix}
$$

Для удобства обозначили ее как X,  но подписали (i), чтобы не перепутать с матрицей. И мы видим, что верхний индекс не меняется (скобки для удобства), всегда i, потому что это одна и та же строка, то есть это ОДИН И ТОТ ЖЕ ЭЗКЕМПЛЯР. Но при этом меняются столбцы (признаки), поэтому нижние индексы 1, 2, 3, 4.

Ну и теперь как выглядит идеальная формальная запись матрицы с признаки объектов (изучи подробно):

$$
X = 
\begin{pmatrix}
x^{(1)}_1 & x^{(1)}_2 & \dots & x^{(1)}_n \\
x^{(2)}_1 & x^{(2)}_2 & \dots & x^{(2)}_n \\
\vdots    & \vdots    &       & \vdots \\
x^{(m)}_1 & x^{(m)}_2 & \dots & x^{(m)}_n
\end{pmatrix}
$$

Так вот теперь возвращаемся к линейной комбинации признаков. Дело в том, что мы будет оперировать со СТРОКАМИ матрицы сейчас, или иными словами с каждым конкретным объектом. Линейная комбинация признаков это запись вида:

$$
z^{(i)} = w_1\,x^{(i)}_{1} + w_2\,x^{(i)}_{2} + \dots + w_n\,x^{(i)}_{n} + b
$$

И так, мы берем итый (или пишут еще i-ый) объект/строку, каждый признак (1, 2... n) умножаем на определенный вес w и в конце прибавляем еще какую-то b. И получаем итый z. Что такое z? это собственно линейная комбинация признаков, линейная потому что нет квадратов, экспонент и так далее, просто умножение и просто плюсик. Такой итый z это значение для КОНКРЕТНОГО экземпляра (строки), для каждой строки он будет свой. Веса, на которые мы умножаем значения признаков помогают регулировать КАКОЙ признак будет более значим, а какой будет МЕНЕЕ значим. Вся задача машинки в простых случаях состоит в том, что у нас первоначально есть ДАННЫЕ в матрице 4х4 (4 признака у 4 экземпляров/объектов):

$$
X =
\begin{pmatrix}
25 & 180 & 75 &  55 \\
30 & 165 & 60 &  40 \\
45 & 190 & 95 & 120 \\
22 & 170 & 68 &  35
\end{pmatrix}
$$

Мы перем первую строчку и умножаем каждый признак на вес:

$$
z^{(1)} = w_1\,25 + w_2\,180 +  w_3\,75 +  w_4\,55
$$

И теперь нам нужно НАЙТИ веса. Это делается тоже определенным образом. Пока для простоты - мы начинаем со случайных весов, получаем какой-то z. И потом мы ВРУЧНУЮ задаем порог. Допустим 0.5. Если финальная линейная комбинация будет больше 0.5, то мы говорим, что этот объект/экземпляр относится к классу 1 (допустим, это котик), а если меньше 0.5, то к классу 2 (собачка). Порог мы задаем самостоятельно, его можно варьировать, но обычно ставят 0.5, потому что веса подстроятся.&#x20;

И вот случайно на первом шаге выбираются веса. Умножаются признаки первого объекта на эти веса, получается первый z. Потом также для второго объекта и для третьего и так далее. ВЕСА ОДИНАКОВЫЕ ДЛЯ ВСЕХ ОБЪЕКТОВ. В итоге у нас получается вектор значений z:

$$
z =
\begin{pmatrix}
0.6 & 0.1 & 0.2 &  0.8 \\
\end{pmatrix}
$$

Каждый объект мы сравниваем с порогом и получаем:

$$
z =
\begin{pmatrix}
котик & собачка & собачка  & котик  \
\end{pmatrix}
$$

но.... у нас есть еще один вектор с ИСТИННЫМИ значениями (меткам) каждого из объектов (мы знаем кто там - котик, собачка)

$$
y=
\begin{pmatrix}
котик & котик & котик & собачка\
\end{pmatrix}
$$

Мы сравниваем и понимаем, что у нас всего 1 из 4 предсказался верно (0.25 точность), что очень мало. Значит наши веса плохие. И модель алгоритм начинает работать дальше, подбирая веса. И так происходит итерационно, он итерационно сравнивает с истинными метками, пока точность не станет максимально возможной, допустим 0.75 (3 из 4 угаданы). Тогда мы останавливаемся и что мы получаем? Мы получаем вектор весов!!

$$
w =
\begin{pmatrix}
0.1 & 0.05 & 0.02 &  0.004 \\
\end{pmatrix}
$$

То есть мы буквально дали определенную значимость каждмоу признаку. И ВОТ НА ДАННОМ ЭТАПЕ МЫ ЗАКОНЧИЛИ ОБУЧЕНИЕ НАШЕЙ МОДЕЛИ.&#x20;

А дальше идет ПРЕДСКАЗАНИЕ. Мы получаем новый объект, экземпляр. У которого мы не знаем какой класс (котики или собачки). Мы также закидываем его и считаем линейную комбинацию

$$
z^{(5)} = 0.1*\,19 + 0.05 *\,130 +  0.02 * \,53 +  0.004 * \,40
$$

И мы получаем какой-то z, допустим 0.89 (наугад), и это больше 0.5, значит это КОШЕЧКА. И мы не знаем правильный ответ, но наша модель предсказала, что это кошечка и мы можем с какой-то доолей вероятности быть в этом уверены.
